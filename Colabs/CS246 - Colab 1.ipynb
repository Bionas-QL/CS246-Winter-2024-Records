{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B92SdhgHcniGPqZ_WhhJgZYmQmz16gBl","timestamp":1735440526749},{"file_id":"1NUy6HpAixV0JER8kJDeWDNlI0hodv2wX","timestamp":1704769604288}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"kPt5q27L5557"},"source":["# CS246 - Colab 1\n","## Word Count in Spark"]},{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's set up Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"k-qHai2252mI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735441653541,"user_tz":300,"elapsed":32581,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}},"outputId":"abd56696-9f9a-4579-f1d9-bba89237dbe3"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive2\n","#the output 'xxx is not a symbolic link' will not affect your implementation or execution\n","#to fix 'xxx is not a symbolic link', you can comment out the lines starting from !mv xxxx\n","#you may need to replace xxx.11 with the correct version if other errors come up after colab update\n","#to get the correct version, use !ls /usr/local/lib to find out\n","!mv /usr/local/lib/libtbbmalloc_proxy.so.2 /usr/local/lib/libtbbmalloc_proxy.so.2.backup\n","!mv /usr/local/lib/libtbbmalloc.so.2 /usr/local/lib/libtbbmalloc.so.2.backup\n","!mv /usr/local/lib/libtbbbind_2_5.so.3 /usr/local/lib/libtbbbind_2_5.so.3.backup\n","!mv /usr/local/lib/libtbb.so.12 /usr/local/lib/libtbb.so.12.backup\n","!mv /usr/local/lib/libtbbbind_2_0.so.3 /usr/local/lib/libtbbbind_2_0.so.3.backup\n","!mv /usr/local/lib/libtbbbind.so.3 /usr/local/lib/libtbbbind.so.3.backup\n","!ln -s /usr/local/lib/libtbbmalloc_proxy.so.2.11 /usr/local/lib/libtbbmalloc_proxy.so.2\n","!ln -s /usr/local/lib/libtbbmalloc.so.2.11 /usr/local/lib/libtbbmalloc.so.2\n","!ln -s /usr/local/lib/libtbbbind_2_5.so.3.11 /usr/local/lib/libtbbbind_2_5.so.3\n","!ln -s /usr/local/lib/libtbb.so.12.11 /usr/local/lib/libtbb.so.12\n","!ln -s /usr/local/lib/libtbbbind_2_0.so.3.11 /usr/local/lib/libtbbbind_2_0.so.3\n","!ln -s /usr/local/lib/libtbbbind.so.3.11 /usr/local/lib/libtbbbind.so.3\n","# !sudo ldconfig\n","#If error related to the above execution occurs, you can try commenting out the above 12 lines under pip install PyDrive2 (not included)\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#the output 'xxx is not a symbolic link' will not affect your implementation or execution\n","#to fix 'xxx is not a symbolic link', you can comment out the lines starting from !mv xxxx\n","#you may need to replace xxx.11 with the correct version if other errors come up after colab update\n","#to get the correct version, use !ls /usr/local/lib to find out\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","The following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n","  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 39.7 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 123634 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"-CJ71AKe91eh"},"source":["Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"5K93ABEy9Zlo","executionInfo":{"status":"ok","timestamp":1735441670822,"user_tz":300,"elapsed":11646,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}}},"source":["from pydrive2.auth import GoogleAuth\n","from pydrive2.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"0orRvrc1-545","executionInfo":{"status":"ok","timestamp":1735441677035,"user_tz":300,"elapsed":3115,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}}},"source":["id='1SE6k_0YukzGd5wK-E4i6mG83nydlfvSa'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('pg100.txt')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwtlO4_m_LbQ"},"source":["If you executed the cells above, you should be able to see the file *pg100.txt* under the \"Files\" tab on the left panel."]},{"cell_type":"markdown","metadata":{"id":"CRaF2A_j_nC7"},"source":["### Your task"]},{"cell_type":"markdown","metadata":{"id":"ebLNUxP0_8x3"},"source":["If you successfully run the setup stage, you are ready to work on the *pg100.txt* file which contains a copy of the complete works of Shakespeare.\n","\n","Write a Spark application which outputs the number of words that start with each letter. This means that for every letter, we want to count the total number of (non-unique) words that start with a specific letter. (If a specific (aka unique) word that starts with letter 'a' appears N times, it should be counted in words starting with 'a' N times.)\n","\n","In your implementation, **ignore the letter case**, i.e., consider all words as lower case. Also, you can ignore all words that **start** with a non-alphabetic character. You should output word counts for the **entire document**, inclusive of the title, author, and the main texts. If you encounter words broken as a result of new lines, e.g. \"pro-ject\" where the segment after the dash sign is on a new line, no special processing is needed and you can safely consider it as two words (\"pro\" and \"ject\").\n","\n","Your outputs will be graded on a range -- if your differences from the ground-truths are within an error threshold of 5, you'll be considered correct.\n","\n","**Hint:**\n","1. split only on space (' ') but not hyphen/dash ('-') or other symbols.\n","2. you may find spark functions explode (https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.explode.html) and split (https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) helpful, but you don't need to restrict to them as long as you can satisfy your goal.\n","\n","Clarification:\n","\n","1. If a word 'project' is separated into two lines in the form of 'pro-' in the first line and 'ject' in the second line, it should be treated as two words (when you import the text using spark.read.text, it treats each newline as a new row in the DataFrame). However, for the word 'self-love' that appears in a single line, it should be treated as one word starting with letter 's'.\n","\n"]},{"cell_type":"code","metadata":{"id":"xu-e7Ph2_ruG","executionInfo":{"status":"ok","timestamp":1735441949383,"user_tz":300,"elapsed":8627,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}}},"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","import pandas as pd\n","\n","# create the Spark Session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# create the Spark Context\n","sc = spark.sparkContext"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"8b8PVip6cKI1"}},{"cell_type":"code","metadata":{"id":"AuAxGFPFB43Y","executionInfo":{"status":"ok","timestamp":1735442000647,"user_tz":300,"elapsed":1637,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}}},"source":["# YOUR\n","txt = spark.read.text(\"pg100.txt\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRFERXNfcMT8","executionInfo":{"status":"ok","timestamp":1735442642518,"user_tz":300,"elapsed":119,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}},"outputId":"fd5be7ef-65f3-4693-93bf-4c9ad0f03fc1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- value: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# CODE\n","words = txt.select(split(txt.value, \" \").alias('value'))\n","words = words.select(explode(words.value).alias('value'))\n","words = words.where((words.value != \"\") & (\"a\" <= lower(words.value)) & (lower(words.value) <= \"z\"))\n","words = words.select(lower(words.value.substr(0, 1)).alias('char'))\n","words.groupBy('char').agg(count('*').alias('numbers')).sort(asc('char')).show()\n"],"metadata":{"id":"fU-euN6nX9-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735444706009,"user_tz":300,"elapsed":1461,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}},"outputId":"008e5140-4c7e-4c6f-bd75-b9ed5faa691f"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-------+\n","|char|numbers|\n","+----+-------+\n","|   a|  84836|\n","|   b|  45455|\n","|   c|  34567|\n","|   d|  29713|\n","|   e|  18697|\n","|   f|  36814|\n","|   g|  20782|\n","|   h|  60563|\n","|   i|  62167|\n","|   j|   3339|\n","|   k|   9418|\n","|   l|  29569|\n","|   m|  55676|\n","|   n|  26759|\n","|   o|  43494|\n","|   p|  27759|\n","|   q|   2377|\n","|   r|  14265|\n","|   s|  65705|\n","|   t| 123602|\n","+----+-------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"7jDCs412ZZcF","executionInfo":{"status":"ok","timestamp":1735444709923,"user_tz":300,"elapsed":122,"user":{"displayName":"Qihang Li","userId":"17460419467500619637"}}},"source":["# HERE\n","\n"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIrXJyVNP2AI"},"source":["Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"]}]}